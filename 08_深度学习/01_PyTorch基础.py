#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
============================================================================
æ¨¡å—åç§°ï¼šPyTorch åŸºç¡€
å­¦ä¹ ç›®æ ‡ï¼šæŒæ¡ PyTorch å¼ é‡æ“ä½œå’Œè‡ªåŠ¨æ±‚å¯¼æœºåˆ¶
PyCharm æŠ€å·§ï¼šå­¦ä¹ ä½¿ç”¨ GPU åŠ é€Ÿè°ƒè¯•
============================================================================
"""

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šPyTorch ç®€ä»‹
# ============================================================================
"""
ã€æ¦‚å¿µè®²è§£ã€‘
PyTorch æ˜¯ Facebook å¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç‰¹ç‚¹ï¼š
1. åŠ¨æ€è®¡ç®—å›¾ - æ›´çµæ´»ï¼Œä¾¿äºè°ƒè¯•
2. GPU åŠ é€Ÿ - æ”¯æŒ CUDA
3. Pythonic - ä¸ NumPy ç±»ä¼¼çš„ API
4. ç”Ÿæ€ä¸°å¯Œ - TorchVisionã€TorchText ç­‰

å®‰è£…ï¼š
CPU: pip install torch torchvision
GPU: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
"""

import torch
import numpy as np

print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
    print(f"GPU åç§°: {torch.cuda.get_device_name(0)}")

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šå¼ é‡ï¼ˆTensorï¼‰
# ============================================================================
"""
ã€æ¦‚å¿µè®²è§£ã€‘
å¼ é‡æ˜¯ PyTorch çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œç±»ä¼¼äº NumPy çš„ ndarrayï¼Œ
ä½†æ”¯æŒ GPU åŠ é€Ÿå’Œè‡ªåŠ¨æ±‚å¯¼ã€‚
"""

# ----------------------------------------------------------------------------
# åˆ›å»ºå¼ é‡
# ----------------------------------------------------------------------------

# ä»åˆ—è¡¨åˆ›å»º
x = torch.tensor([1, 2, 3, 4, 5])
print(f"ä¸€ç»´å¼ é‡: {x}")
print(f"å½¢çŠ¶: {x.shape}")
print(f"æ•°æ®ç±»å‹: {x.dtype}")

# ä» NumPy åˆ›å»º
np_array = np.array([1, 2, 3])
tensor_from_numpy = torch.from_numpy(np_array)
print(f"ä» NumPy åˆ›å»º: {tensor_from_numpy}")

# åˆ›å»ºç‰¹å®šå½¢çŠ¶çš„å¼ é‡
zeros = torch.zeros(3, 4)  # å…¨é›¶
ones = torch.ones(2, 3)    # å…¨ä¸€
random = torch.rand(2, 3)  # éšæœº [0, 1)
randn = torch.randn(2, 3)  # æ ‡å‡†æ­£æ€åˆ†å¸ƒ

print(f"å…¨é›¶å¼ é‡:\n{zeros}")
print(f"éšæœºå¼ é‡:\n{random}")

# åˆ›å»ºåºåˆ—
arange = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]
linspace = torch.linspace(0, 1, 5)  # å‡åŒ€åˆ†å¸ƒ

print(f"arange: {arange}")
print(f"linspace: {linspace}")

# æŒ‡å®šæ•°æ®ç±»å‹
float_tensor = torch.tensor([1, 2, 3], dtype=torch.float32)
long_tensor = torch.tensor([1, 2, 3], dtype=torch.long)

print(f"float32: {float_tensor.dtype}")
print(f"long: {long_tensor.dtype}")

# ----------------------------------------------------------------------------
# å¼ é‡æ“ä½œ
# ----------------------------------------------------------------------------

# ç´¢å¼•å’Œåˆ‡ç‰‡
x = torch.arange(12).reshape(3, 4)
print(f"äºŒç»´å¼ é‡:\n{x}")
print(f"ç¬¬ä¸€è¡Œ: {x[0]}")
print(f"ç¬¬ä¸€åˆ—: {x[:, 0]}")
print(f"åˆ‡ç‰‡: {x[0:2, 1:3]}")

# å½¢çŠ¶æ“ä½œ
x = torch.arange(12)
print(f"åŸå§‹å½¢çŠ¶: {x.shape}")

# reshape - æ”¹å˜å½¢çŠ¶
x_reshaped = x.reshape(3, 4)
print(f"reshape: {x_reshaped.shape}")

# view - å…±äº«å†…å­˜çš„ reshape
x_view = x.view(2, 6)
print(f"view: {x_view.shape}")

# squeeze - å»é™¤å¤§å°ä¸º1çš„ç»´åº¦
x = torch.randn(1, 3, 1, 4)
print(f"squeeze å‰: {x.shape}")
x_squeezed = x.squeeze()
print(f"squeeze å: {x_squeezed.shape}")

# unsqueeze - æ·»åŠ ç»´åº¦
x = torch.randn(3, 4)
x_unsqueezed = x.unsqueeze(0)  # åœ¨ç¬¬0ç»´æ·»åŠ 
print(f"unsqueeze å: {x_unsqueezed.shape}")

# è½¬ç½®
x = torch.randn(2, 3)
print(f"è½¬ç½®å‰:\n{x}")
print(f"è½¬ç½®å:\n{x.T}")

# çŸ©é˜µä¹˜æ³•
a = torch.randn(2, 3)
b = torch.randn(3, 4)
c = torch.mm(a, b)  # æˆ– a @ b
print(f"çŸ©é˜µä¹˜æ³•ç»“æœå½¢çŠ¶: {c.shape}")

# ----------------------------------------------------------------------------
# æ•°å­¦è¿ç®—
# ----------------------------------------------------------------------------

x = torch.tensor([1.0, 2.0, 3.0])
y = torch.tensor([4.0, 5.0, 6.0])

# åŸºæœ¬è¿ç®—
print(f"åŠ æ³•: {x + y}")
print(f"å‡æ³•: {x - y}")
print(f"ä¹˜æ³•: {x * y}")
print(f"é™¤æ³•: {x / y}")

# æ•°å­¦å‡½æ•°
x = torch.tensor([0.0, 0.5, 1.0])
print(f"sin: {torch.sin(x)}")
print(f"cos: {torch.cos(x)}")
print(f"exp: {torch.exp(x)}")
print(f"log: {torch.log(x + 1)}")  # é¿å… log(0)
print(f"sqrt: {torch.sqrt(x)}")

# èšåˆæ“ä½œ
x = torch.randn(3, 4)
print(f"æ±‚å’Œ: {x.sum()}")
print(f"å‡å€¼: {x.mean()}")
print(f"æœ€å¤§å€¼: {x.max()}")
print(f"æœ€å°å€¼: {x.min()}")

# æ²¿ç»´åº¦èšåˆ
print(f"æŒ‰è¡Œæ±‚å’Œ: {x.sum(dim=1)}")
print(f"æŒ‰åˆ—æ±‚å‡å€¼: {x.mean(dim=0)}")

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šè‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰
# ============================================================================
"""
ã€æ¦‚å¿µè®²è§£ã€‘
Autograd æ˜¯ PyTorch çš„è‡ªåŠ¨å¾®åˆ†å¼•æ“ï¼Œå¯ä»¥è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ã€‚
åªéœ€è¦è®¾ç½® requires_grad=Trueï¼ŒPyTorch ä¼šè·Ÿè¸ªæ‰€æœ‰æ“ä½œã€‚
"""

# ----------------------------------------------------------------------------
# åŸºæœ¬è‡ªåŠ¨æ±‚å¯¼
# ----------------------------------------------------------------------------

# åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡
x = torch.tensor([2.0], requires_grad=True)
print(f"x: {x}")
print(f"requires_grad: {x.requires_grad}")

# å®šä¹‰è®¡ç®—
y = x ** 2
z = y + 3

print(f"y = x^2 = {y}")
print(f"z = y + 3 = {z}")

# åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
z.backward()

# dz/dx = dz/dy * dy/dx = 1 * 2x = 2x = 4
print(f"æ¢¯åº¦ dz/dx: {x.grad}")

# ----------------------------------------------------------------------------
# å¤šå˜é‡æ±‚å¯¼
# ----------------------------------------------------------------------------

x = torch.tensor([1.0], requires_grad=True)
y = torch.tensor([2.0], requires_grad=True)

z = x * y + x ** 2
z.backward()

print(f"dz/dx = y + 2x = {x.grad}")  # y + 2x = 2 + 2 = 4
print(f"dz/dy = x = {y.grad}")       # x = 1

# ----------------------------------------------------------------------------
# åœæ­¢æ¢¯åº¦è·Ÿè¸ª
# ----------------------------------------------------------------------------

x = torch.tensor([1.0], requires_grad=True)

# æ–¹æ³•1ï¼šdetach()
y = x ** 2
y_detached = y.detach()  # åˆ›å»ºä¸éœ€è¦æ¢¯åº¦çš„æ–°å¼ é‡

# æ–¹æ³•2ï¼štorch.no_grad()
with torch.no_grad():
    z = x ** 2
    print(f"no_grad ä¸­çš„ z.requires_grad: {z.requires_grad}")

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šGPU åŠ é€Ÿ
# ============================================================================

# ----------------------------------------------------------------------------
# è®¾å¤‡ç®¡ç†
# ----------------------------------------------------------------------------

# æ£€æŸ¥è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# å°†å¼ é‡ç§»åŠ¨åˆ° GPU
x = torch.randn(3, 3)
x_gpu = x.to(device)
print(f"GPU å¼ é‡è®¾å¤‡: {x_gpu.device}")

# åœ¨ GPU ä¸Šè¿›è¡Œè¿ç®—
if torch.cuda.is_available():
    y_gpu = torch.randn(3, 3).to(device)
    z_gpu = x_gpu + y_gpu
    print(f"GPU è¿ç®—ç»“æœ:\n{z_gpu}")
    
    # ç§»å› CPU
    z_cpu = z_gpu.cpu()
    # æˆ– z_cpu = z_gpu.to('cpu')

# ----------------------------------------------------------------------------
# æ€§èƒ½å¯¹æ¯”
# ----------------------------------------------------------------------------

import time

def matrix_multiply_test(device, size=1000):
    """çŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯•"""
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)
    
    # é¢„çƒ­
    c = a @ b
    
    # è®¡æ—¶
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    start = time.time()
    for _ in range(10):
        c = a @ b
    
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    elapsed = time.time() - start
    return elapsed

# CPU æµ‹è¯•
cpu_time = matrix_multiply_test(torch.device('cpu'))
print(f"CPU æ—¶é—´: {cpu_time:.4f}ç§’")

# GPU æµ‹è¯•
if torch.cuda.is_available():
    gpu_time = matrix_multiply_test(torch.device('cuda'))
    print(f"GPU æ—¶é—´: {gpu_time:.4f}ç§’")
    print(f"GPU åŠ é€Ÿæ¯”: {cpu_time / gpu_time:.1f}x")

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½
# ============================================================================

from torch.utils.data import Dataset, DataLoader

# ----------------------------------------------------------------------------
# è‡ªå®šä¹‰æ•°æ®é›†
# ----------------------------------------------------------------------------

class CustomDataset(Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ç¤ºä¾‹"""
    
    def __init__(self, size=100):
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®
        self.x = torch.randn(size, 10)
        self.y = torch.randint(0, 2, (size,))
    
    def __len__(self):
        return len(self.x)
    
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
dataset = CustomDataset(size=100)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
print(f"æ‰¹æ¬¡æ•°: {len(dataloader)}")

# éå†æ•°æ®
for batch_x, batch_y in dataloader:
    print(f"æ‰¹æ¬¡ x å½¢çŠ¶: {batch_x.shape}")
    print(f"æ‰¹æ¬¡ y å½¢çŠ¶: {batch_y.shape}")
    break

# ============================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šç®€å•ç¥ç»ç½‘ç»œ
# ============================================================================

import torch.nn as nn
import torch.optim as optim

# ----------------------------------------------------------------------------
# å®šä¹‰ç¥ç»ç½‘ç»œ
# ----------------------------------------------------------------------------

class SimpleNet(nn.Module):
    """ç®€å•ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_size, hidden_size, num_classes):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

# åˆ›å»ºæ¨¡å‹
model = SimpleNet(input_size=10, hidden_size=20, num_classes=2)
print(f"æ¨¡å‹ç»“æ„:\n{model}")

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ----------------------------------------------------------------------------
# è®­ç»ƒå¾ªç¯
# ----------------------------------------------------------------------------

def train_model(model, dataloader, criterion, optimizer, num_epochs=10):
    """è®­ç»ƒæ¨¡å‹"""
    model.train()
    
    for epoch in range(num_epochs):
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_x, batch_y in dataloader:
            # å‰å‘ä¼ æ’­
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        avg_loss = total_loss / len(dataloader)
        accuracy = 100 * correct / total
        
        print(f"Epoch [{epoch+1}/{num_epochs}], "
              f"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")

# è®­ç»ƒ
print("\nå¼€å§‹è®­ç»ƒ:")
train_model(model, dataloader, criterion, optimizer, num_epochs=5)

# ----------------------------------------------------------------------------
# ä¿å­˜å’ŒåŠ è½½æ¨¡å‹
# ----------------------------------------------------------------------------

# ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), 'simple_net.pth')
print("æ¨¡å‹å·²ä¿å­˜")

# åŠ è½½æ¨¡å‹
loaded_model = SimpleNet(input_size=10, hidden_size=20, num_classes=2)
loaded_model.load_state_dict(torch.load('simple_net.pth'))
loaded_model.eval()
print("æ¨¡å‹å·²åŠ è½½")

# æ¸…ç†
import os
if os.path.exists('simple_net.pth'):
    os.remove('simple_net.pth')

# ============================================================================
# æœ¬èŠ‚å°ç»“
# ============================================================================
"""
âœ… æŒæ¡çš„çŸ¥è¯†ç‚¹ï¼š
1. PyTorch å®‰è£…å’Œç¯å¢ƒé…ç½®
2. å¼ é‡çš„åˆ›å»ºå’Œæ“ä½œ
3. è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶
4. GPU åŠ é€Ÿä½¿ç”¨
5. æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
6. ç¥ç»ç½‘ç»œå®šä¹‰å’Œè®­ç»ƒ
7. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

ğŸ”§ PyCharm æŠ€å·§ï¼š
1. ä½¿ç”¨ CUDA è°ƒè¯•
2. Variables é¢æ¿æŸ¥çœ‹å¼ é‡
3. Structure é¢æ¿æŸ¥çœ‹æ¨¡å‹ç»“æ„
4. ä½¿ç”¨ Scientific Mode æŸ¥çœ‹å›¾è¡¨

â¡ï¸ ä¸‹ä¸€èŠ‚ï¼šç¥ç»ç½‘ç»œæ·±å…¥
"""

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("PyTorch åŸºç¡€æ¨¡å—å­¦ä¹ å®Œæˆï¼")
    print("=" * 60)
