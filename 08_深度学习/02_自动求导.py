#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
============================================================================
模块名称：自动求导
学习目标：理解 PyTorch 的自动求导机制
PyCharm 技巧：学习调试梯度计算
============================================================================
"""

import torch

# ============================================================================
# 第一部分：自动求导概念
# ============================================================================
"""
【概念讲解】
PyTorch 的 autograd 模块实现了自动微分，
可以自动计算张量的梯度，这是深度学习的基础。

核心概念：
- Tensor：张量，可以设置 requires_grad=True 来追踪计算历史
- grad_fn：记录张量是如何创建的
- backward()：反向传播，计算梯度
- grad：存储梯度值
"""

# ============================================================================
# 第二部分：张量与梯度
# ============================================================================

# ----------------------------------------------------------------------------
# 创建需要梯度的张量
# ----------------------------------------------------------------------------

# requires_grad=True 表示需要计算梯度
x = torch.tensor([2.0, 3.0], requires_grad=True)
print(f"张量 x: {x}")
print(f"需要梯度: {x.requires_grad}")
print(f"梯度函数: {x.grad_fn}")  # None，因为 x 是叶子节点

# ----------------------------------------------------------------------------
# 计算梯度
# ----------------------------------------------------------------------------

# 定义计算
y = x ** 2      # y = x^2
z = y.sum()     # z = sum(y)

print(f"\ny = x^2: {y}")
print(f"z = sum(y): {z}")

# 反向传播
z.backward()

# 查看梯度
# dy/dx = 2x，所以梯度应该是 [4, 6]
print(f"\n梯度 x.grad: {x.grad}")

# ----------------------------------------------------------------------------
# 梯度计算原理
# ----------------------------------------------------------------------------

"""
【数学解释】
z = sum(x^2)
dz/dx = 2x

当 x = [2, 3] 时
grad = [2*2, 2*3] = [4, 6]
"""

# ============================================================================
# 第三部分：计算图
# ============================================================================

# 清除之前的梯度
x = torch.tensor([2.0, 3.0], requires_grad=True)

# 构建计算图
a = x + 2       # a = x + 2
b = a * a       # b = a^2 = (x+2)^2
c = b.mean()    # c = mean(b)

print(f"\n计算图:")
print(f"a = x + 2: {a}")
print(f"b = a^2: {b}")
print(f"c = mean(b): {c}")

# 查看 grad_fn
print(f"\ngrad_fn:")
print(f"a.grad_fn: {a.grad_fn}")
print(f"b.grad_fn: {b.grad_fn}")
print(f"c.grad_fn: {c.grad_fn}")

# 反向传播
c.backward()

print(f"\n梯度: {x.grad}")
# dc/dx = 2(x+2)/2 = x+2
# grad = [2+2, 3+2] = [4, 5]

# ============================================================================
# 第四部分：梯度控制
# ============================================================================

# ----------------------------------------------------------------------------
# 阻止梯度追踪
# ----------------------------------------------------------------------------

x = torch.tensor([2.0], requires_grad=True)

# 方式1：torch.no_grad()
with torch.no_grad():
    y = x * 2
    print(f"\nno_grad 中的 y.requires_grad: {y.requires_grad}")

# 方式2：detach()
y = x * 2
y_detached = y.detach()
print(f"detach 后 requires_grad: {y_detached.requires_grad}")

# ----------------------------------------------------------------------------
# 清除梯度
# ----------------------------------------------------------------------------

x = torch.tensor([1.0, 2.0], requires_grad=True)

# 多次反向传播
y = x.sum()
y.backward()
print(f"第一次梯度: {x.grad}")

# 梯度会累加
y = x.sum()
y.backward()
print(f"累加后梯度: {x.grad}")  # [2, 4] 而不是 [1, 2]

# 清除梯度
x.grad.zero_()
print(f"清除后梯度: {x.grad}")

# ============================================================================
# 第五部分：非标量输出
# ============================================================================

x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)

# 非标量需要传入 gradient 参数
y = x ** 2

# 计算梯度
# gradient 参数指定对各输出的权重
y.backward(torch.ones_like(y))

print(f"\n非标量梯度:\n{x.grad}")
# grad = 2x = [[2, 4], [6, 8]]

# ============================================================================
# 第六部分：实际应用示例
# ============================================================================

# ----------------------------------------------------------------------------
# 线性回归梯度
# ----------------------------------------------------------------------------

# 数据
x_data = torch.tensor([1.0, 2.0, 3.0, 4.0])
y_data = torch.tensor([2.0, 4.0, 6.0, 8.0])

# 参数
w = torch.tensor([0.0], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

# 前向传播
def forward(x):
    return w * x + b

# 损失函数
def loss_fn(y_pred, y_true):
    return ((y_pred - y_true) ** 2).mean()

# 计算损失
y_pred = forward(x_data)
loss = loss_fn(y_pred, y_data)

print(f"\n线性回归:")
print(f"预测值: {y_pred}")
print(f"损失: {loss.item()}")

# 反向传播
loss.backward()

print(f"w 的梯度: {w.grad}")
print(f"b 的梯度: {b.grad}")

# 更新参数（梯度下降）
learning_rate = 0.01
with torch.no_grad():
    w -= learning_rate * w.grad
    b -= learning_rate * b.grad
    w.grad.zero_()
    b.grad.zero_()

print(f"更新后 w: {w.item()}")
print(f"更新后 b: {b.item()}")

# ----------------------------------------------------------------------------
# 完整训练循环
# ----------------------------------------------------------------------------

print("\n训练线性回归:")
w = torch.tensor([0.0], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

for epoch in range(100):
    # 前向传播
    y_pred = forward(x_data)
    loss = loss_fn(y_pred, y_data)
    
    # 反向传播
    w.grad = None
    b.grad = None
    loss.backward()
    
    # 更新参数
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad
    
    if (epoch + 1) % 20 == 0:
        print(f"Epoch {epoch+1}: loss = {loss.item():.4f}, w = {w.item():.4f}, b = {b.item():.4f}")

print(f"\n最终模型: y = {w.item():.2f}x + {b.item():.2f}")
print(f"真实模型: y = 2x + 0")

# ============================================================================
# 本节小结
# ============================================================================
"""
✅ 掌握的知识点：
1. 自动求导原理
2. requires_grad 属性
3. backward() 方法
4. 计算图概念
5. 梯度控制（no_grad, detach）
6. 梯度清零
7. 非标量输出处理

➡️ 下一节：数据加载
"""

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("自动求导模块学习完成！")
    print("=" * 60)
