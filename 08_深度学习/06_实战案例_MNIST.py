#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
============================================================================
æ¨¡å—åç§°ï¼šå®æˆ˜æ¡ˆä¾‹ - MNIST æ‰‹å†™æ•°å­—è¯†åˆ«
å­¦ä¹ ç›®æ ‡ï¼šä½¿ç”¨ PyTorch å®Œæˆå›¾åƒåˆ†ç±»é¡¹ç›®
PyCharm æŠ€å·§ï¼šå­¦ä¹ å®Œæ•´çš„æ·±åº¦å­¦ä¹ é¡¹ç›®å¼€å‘æµç¨‹
============================================================================
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import os

# ============================================================================
# é¡¹ç›®æ¦‚è¿°
# ============================================================================
"""
ã€é¡¹ç›®æè¿°ã€‘
MNIST æ˜¯ç»å…¸çš„æ‰‹å†™æ•°å­—è¯†åˆ«æ•°æ®é›†ï¼š
- 60,000 å¼ è®­ç»ƒå›¾ç‰‡
- 10,000 å¼ æµ‹è¯•å›¾ç‰‡
- å›¾ç‰‡å¤§å°ï¼š28x28 ç°åº¦å›¾
- ç±»åˆ«ï¼š0-9 å…±10ä¸ªæ•°å­—

ç›®æ ‡ï¼šæ„å»º CNN æ¨¡å‹ï¼Œå®ç°é«˜ç²¾åº¦æ•°å­—è¯†åˆ«
"""

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šé…ç½®å’Œå‡†å¤‡
# ============================================================================

# é…ç½®
class Config:
    # æ•°æ®
    batch_size = 64
    num_workers = 0  # Windows è®¾ä¸º 0
    
    # æ¨¡å‹
    num_classes = 10
    
    # è®­ç»ƒ
    num_epochs = 10
    learning_rate = 0.001
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"ä½¿ç”¨è®¾å¤‡: {Config.device}")

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡
# ============================================================================

# æ•°æ®å˜æ¢
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST å‡å€¼å’Œæ ‡å‡†å·®
])

# ä¸‹è½½æ•°æ®é›†
print("ä¸‹è½½ MNIST æ•°æ®é›†...")
train_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=True,
    download=True,
    transform=transform
)

test_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=False,
    download=True,
    transform=transform
)

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
train_size = int(0.9 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(
    train_dataset, [train_size, val_size]
)

# åˆ›å»º DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=Config.batch_size,
    shuffle=True,
    num_workers=Config.num_workers
)

val_loader = DataLoader(
    val_dataset,
    batch_size=Config.batch_size,
    num_workers=Config.num_workers
)

test_loader = DataLoader(
    test_dataset,
    batch_size=Config.batch_size,
    num_workers=Config.num_workers
)

print(f"æ•°æ®é›†å¤§å°: è®­ç»ƒ={len(train_dataset)}, éªŒè¯={len(val_dataset)}, æµ‹è¯•={len(test_dataset)}")

# å¯è§†åŒ–æ ·æœ¬
def show_samples(loader, num_samples=8):
    """æ˜¾ç¤ºæ ·æœ¬å›¾ç‰‡"""
    images, labels = next(iter(loader))
    
    fig, axes = plt.subplots(1, num_samples, figsize=(12, 2))
    for i in range(num_samples):
        img = images[i].squeeze().numpy()
        axes[i].imshow(img, cmap='gray')
        axes[i].set_title(f'Label: {labels[i]}')
        axes[i].axis('off')
    plt.tight_layout()
    plt.savefig('mnist_samples.png', dpi=100)
    plt.close()
    print("æ ·æœ¬å›¾ç‰‡å·²ä¿å­˜åˆ° mnist_samples.png")

show_samples(train_loader)

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®šä¹‰æ¨¡å‹
# ============================================================================

class MNISTClassifier(nn.Module):
    """MNIST åˆ†ç±»å™¨ - CNN"""
    
    def __init__(self, num_classes=10):
        super().__init__()
        
        # å·ç§¯å±‚
        self.conv_layers = nn.Sequential(
            # ç¬¬ä¸€ä¸ªå·ç§¯å—
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 28 -> 14
            
            # ç¬¬äºŒä¸ªå·ç§¯å—
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 14 -> 7
            
            # ç¬¬ä¸‰ä¸ªå·ç§¯å—
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2)   # 7 -> 3
        )
        
        # å…¨è¿æ¥å±‚
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 3 * 3, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.conv_layers(x)
        x = self.fc_layers(x)
        return x

# åˆ›å»ºæ¨¡å‹
model = MNISTClassifier(Config.num_classes).to(Config.device)
print(f"\næ¨¡å‹ç»“æ„:\n{model}")

# è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"æ€»å‚æ•°: {total_params:,}, å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šè®­ç»ƒé…ç½®
# ============================================================================

# æŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss()

# ä¼˜åŒ–å™¨
optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=2
)

# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šè®­ç»ƒå‡½æ•°
# ============================================================================

def train_epoch(model, loader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    return running_loss / len(loader), correct / total

def evaluate(model, loader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    return running_loss / len(loader), correct / total

# ============================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šè®­ç»ƒå¾ªç¯
# ============================================================================

history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
best_val_acc = 0.0

print("\nå¼€å§‹è®­ç»ƒ:")
for epoch in range(Config.num_epochs):
    # è®­ç»ƒ
    train_loss, train_acc = train_epoch(
        model, train_loader, criterion, optimizer, Config.device
    )
    
    # éªŒè¯
    val_loss, val_acc = evaluate(
        model, val_loader, criterion, Config.device
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(val_loss)
    
    # è®°å½•
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc,
        }, 'best_mnist_model.pth')
    
    # æ‰“å°è¿›åº¦
    print(f"Epoch [{epoch+1}/{Config.num_epochs}] "
          f"Train: Loss={train_loss:.4f}, Acc={train_acc:.4f} | "
          f"Val: Loss={val_loss:.4f}, Acc={val_acc:.4f}")

# ============================================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šè¯„ä¼°å’Œå¯è§†åŒ–
# ============================================================================

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

axes[0].plot(history['train_loss'], label='Train')
axes[0].plot(history['val_loss'], label='Val')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Loss Curve')
axes[0].legend()

axes[1].plot(history['train_acc'], label='Train')
axes[1].plot(history['val_acc'], label='Val')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy')
axes[1].set_title('Accuracy Curve')
axes[1].legend()

plt.tight_layout()
plt.savefig('mnist_training.png', dpi=100)
plt.close()
print("\nè®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° mnist_training.png")

# æµ‹è¯•æœ€ä½³æ¨¡å‹
checkpoint = torch.load('best_mnist_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
test_loss, test_acc = evaluate(model, test_loader, criterion, Config.device)
print(f"\næµ‹è¯•ç»“æœ: Loss={test_loss:.4f}, Accuracy={test_acc:.4f}")

# ============================================================================
# ç¬¬å…«éƒ¨åˆ†ï¼šé¢„æµ‹ç¤ºä¾‹
# ============================================================================

def predict_and_show(model, loader, device, num_samples=10):
    """é¢„æµ‹å¹¶æ˜¾ç¤ºç»“æœ"""
    model.eval()
    images, labels = next(iter(loader))
    images = images[:num_samples].to(device)
    labels = labels[:num_samples]
    
    with torch.no_grad():
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
    
    # æ˜¾ç¤º
    fig, axes = plt.subplots(2, 5, figsize=(12, 5))
    for i, ax in enumerate(axes.flat):
        img = images[i].cpu().squeeze().numpy()
        ax.imshow(img, cmap='gray')
        color = 'green' if predicted[i] == labels[i] else 'red'
        ax.set_title(f'Pred: {predicted[i].item()}, True: {labels[i]}', color=color)
        ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('mnist_predictions.png', dpi=100)
    plt.close()
    print("é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° mnist_predictions.png")

predict_and_show(model, test_loader, Config.device)

# ============================================================================
# æ¸…ç†
# ============================================================================

for f in ['best_mnist_model.pth', 'mnist_samples.png', 'mnist_training.png', 'mnist_predictions.png']:
    if os.path.exists(f):
        os.remove(f)

# ============================================================================
# æœ¬èŠ‚å°ç»“
# ============================================================================
"""
âœ… é¡¹ç›®æ€»ç»“ï¼š
1. å®Œæ•´çš„æ·±åº¦å­¦ä¹ é¡¹ç›®æµç¨‹
2. CNN æ¨¡å‹è®¾è®¡
3. æ•°æ®é¢„å¤„ç†å’Œå¢å¼º
4. è®­ç»ƒå’ŒéªŒè¯
5. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
6. ç»“æœå¯è§†åŒ–

ğŸ‰ æ­å–œå®Œæˆæ·±åº¦å­¦ä¹ æ¨¡å—ï¼
"""

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("MNIST å®æˆ˜æ¡ˆä¾‹å­¦ä¹ å®Œæˆï¼")
    print("=" * 60)
